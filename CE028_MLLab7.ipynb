{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1ForhqNcuc9NES_XRj9OeXeLFuqPWJTIh",
      "authorship_tag": "ABX9TyOGI7PmyKFaXWa18qKaKm0+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Implementation of Logistic Regression\n"
      ],
      "metadata": {
        "id": "EnYFgLq1sZrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sfVF6nJHqV4A"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the csv file, del 2 columns from the file, checking first few rows of the file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/datasets/BuyComputer.csv\")\n",
        "data.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "data.tail()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "oVthENqcsv5x",
        "outputId": "f795796c-b127-4c4f-bfa9-4f28b5cd9265"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Age  EstimatedSalary  Purchased\n",
              "395   46            41000          1\n",
              "396   51            23000          1\n",
              "397   50            20000          1\n",
              "398   36            33000          0\n",
              "399   49            36000          1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d786ac54-223e-4d6f-be94-854e749972bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>EstimatedSalary</th>\n",
              "      <th>Purchased</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>46</td>\n",
              "      <td>41000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>51</td>\n",
              "      <td>23000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>50</td>\n",
              "      <td>20000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>36</td>\n",
              "      <td>33000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>49</td>\n",
              "      <td>36000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d786ac54-223e-4d6f-be94-854e749972bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d786ac54-223e-4d6f-be94-854e749972bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d786ac54-223e-4d6f-be94-854e749972bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Declaring X as all columns excluding last\n",
        "X = data.iloc[:,:-1].values\n",
        "\n",
        "#Declare label as last column in the source file\n",
        "Y = data.iloc[:,-1].values"
      ],
      "metadata": {
        "id": "QMtZpUc3uMpJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mvBz39GuSYS",
        "outputId": "499f0910-c39e-47e6-ede0-bfcfb635d245"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
              "       0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
              "       1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0,\n",
              "       1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
              "       0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1,\n",
              "       1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
              "       0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0,\n",
              "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1,\n",
              "       0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1,\n",
              "       1, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=28)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "# Sacaling data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hj2A1Q-87ekZ",
        "outputId": "4ceb6611-ac33-46af-b8ac-f6e5fb0efea8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300, 2)\n",
            "(100, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "metadata": {
        "id": "eeqYlgh9uq_v"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Variabes to calculate sigmoid function\n",
        "y_pred = []\n",
        "len_x = len(X_train[0])\n",
        "w = []\n",
        "b = 0.2\n",
        "print(len_x)\n",
        "entries = len(X_train[:,0])\n",
        "\n",
        "for weights in range(len_x):\n",
        "    w.append(0)\n",
        "w"
      ],
      "metadata": {
        "id": "LtMnqvC-vDBn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4809142-c28e-4564-b98a-b9ea70c2b8d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1/(1 + np.exp(-z))\n",
        "#Prediction\n",
        "def predict(input):\n",
        "    z = np.dot(input, w) + b\n",
        "    h= sigmoid(z)\n",
        "    for i in range(len(h)):\n",
        "        if(h[i]>=0.5):\n",
        "            h[i]=1\n",
        "        else:\n",
        "            h[i]=0\n",
        "    return h\n",
        "    #Loss function\n",
        "def loss_func(y, y1):\n",
        "    total_bce_loss = np.sum(-y * np.log(y1) - (1 - y) * np.log(1 - y1))\n",
        "    m = y.shape[0]\n",
        "    j = total_bce_loss /m\n",
        "    return j"
      ],
      "metadata": {
        "id": "tTQtHbr51Gne"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dw = []\n",
        "db = 0\n",
        "J = 0\n",
        "alpha = 0.1\n",
        "for x in range(len_x):\n",
        "    dw.append(0)"
      ],
      "metadata": {
        "id": "vgCe8CDN76YW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Repeating the process 300 times\n",
        "for i in range(300):\n",
        "    z = np.dot(X_train, w) + b\n",
        "    y_pred = sigmoid(z)\n",
        "    l = loss_func(y_pred, y_train)\n",
        "    dw = np.dot((y_pred-y_train).T, X_train)/X_train.shape[0]\n",
        "    db = np.mean(y_pred-y_train)\n",
        "    w = w - alpha * dw\n",
        "    b = b - alpha* db\n",
        "    print(\"Round:\",i,\"Weight:\",w,\"Bias:\",b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5T3WB_q8Dci",
        "outputId": "f7e1cb50-27cc-4a6a-f38c-7eac245b7b65"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-1622761bf0c7>:16: RuntimeWarning: divide by zero encountered in log\n",
            "  total_bce_loss = np.sum(-y * np.log(y1) - (1 - y) * np.log(1 - y1))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round: 0 Weight: [0.03069456 0.01910722] Bias: 0.18234993360208554\n",
            "Round: 1 Weight: [0.06054473 0.03760655] Bias: 0.16513884285260955\n",
            "Round: 2 Weight: [0.08957358 0.05551683] Bias: 0.14835927823399458\n",
            "Round: 3 Weight: [0.11780518 0.07285753] Bias: 0.13200287723885462\n",
            "Round: 4 Weight: [0.1452643  0.08964854] Bias: 0.1160605212741687\n",
            "Round: 5 Weight: [0.17197617 0.10590996] Bias: 0.10052248560409181\n",
            "Round: 6 Weight: [0.1979662  0.12166189] Bias: 0.08537857935391886\n",
            "Round: 7 Weight: [0.22325975 0.13692426] Bias: 0.07061827356975754\n",
            "Round: 8 Weight: [0.24788199 0.15171675] Bias: 0.05623081618455358\n",
            "Round: 9 Weight: [0.27185771 0.16605864] Bias: 0.04220533345388654\n",
            "Round: 10 Weight: [0.29521123 0.17996869] Bias: 0.028530917986796446\n",
            "Round: 11 Weight: [0.31796624 0.19346516] Bias: 0.015196703914389785\n",
            "Round: 12 Weight: [0.3401458  0.20656568] Bias: 0.002191930027629234\n",
            "Round: 13 Weight: [0.36177223 0.21928724] Bias: -0.010494008104383307\n",
            "Round: 14 Weight: [0.38286708 0.23164617] Bias: -0.02287151593180546\n",
            "Round: 15 Weight: [0.40345111 0.24365816] Bias: -0.03495076650776023\n",
            "Round: 16 Weight: [0.42354429 0.25533818] Bias: -0.046741674752204025\n",
            "Round: 17 Weight: [0.44316576 0.26670055] Bias: -0.058253878346484246\n",
            "Round: 18 Weight: [0.46233388 0.27775895] Bias: -0.06949672428686696\n",
            "Round: 19 Weight: [0.48106621 0.28852637] Bias: -0.08047926021458668\n",
            "Round: 20 Weight: [0.49937953 0.29901523] Bias: -0.09121022973365994\n",
            "Round: 21 Weight: [0.51728989 0.3092373 ] Bias: -0.10169807102076638\n",
            "Round: 22 Weight: [0.53481259 0.31920378] Bias: -0.11195091812054084\n",
            "Round: 23 Weight: [0.55196224 0.32892534] Bias: -0.12197660440251755\n",
            "Round: 24 Weight: [0.56875276 0.33841207] Bias: -0.13178266773155495\n",
            "Round: 25 Weight: [0.58519744 0.34767358] Bias: -0.1413763569713712\n",
            "Round: 26 Weight: [0.60130894 0.356719  ] Bias: -0.1507646395008317\n",
            "Round: 27 Weight: [0.61709932 0.36555697] Bias: -0.15995420947516542\n",
            "Round: 28 Weight: [0.63258009 0.37419573] Bias: -0.16895149660985156\n",
            "Round: 29 Weight: [0.64776222 0.38264309] Bias: -0.17776267530412174\n",
            "Round: 30 Weight: [0.66265615 0.39090644] Bias: -0.18639367395451856\n",
            "Round: 31 Weight: [0.67727186 0.39899285] Bias: -0.19485018433739038\n",
            "Round: 32 Weight: [0.69161885 0.40690901] Bias: -0.20313767096321142\n",
            "Round: 33 Weight: [0.7057062  0.41466127] Bias: -0.2112613803257773\n",
            "Round: 34 Weight: [0.71954255 0.42225569] Bias: -0.2192263499861692\n",
            "Round: 35 Weight: [0.73313618 0.42969803] Bias: -0.2270374174453816\n",
            "Round: 36 Weight: [0.74649496 0.43699375] Bias: -0.23469922877108784\n",
            "Round: 37 Weight: [0.75962643 0.44414807] Bias: -0.2422162469535442\n",
            "Round: 38 Weight: [0.7725378  0.45116596] Bias: -0.24959275997342759\n",
            "Round: 39 Weight: [0.78523594 0.45805212] Bias: -0.25683288857073616\n",
            "Round: 40 Weight: [0.79772743 0.46481109] Bias: -0.2639405937089983\n",
            "Round: 41 Weight: [0.81001857 0.47144714] Bias: -0.2709196837331304\n",
            "Round: 42 Weight: [0.82211538 0.47796437] Bias: -0.27777382122253164\n",
            "Round: 43 Weight: [0.83402364 0.4843667 ] Bias: -0.2845065295435484\n",
            "Round: 44 Weight: [0.84574887 0.49065786] Bias: -0.29112119910740575\n",
            "Round: 45 Weight: [0.85729635 0.49684141] Bias: -0.297621093341189\n",
            "Round: 46 Weight: [0.86867118 0.50292075] Bias: -0.30400935438055393\n",
            "Round: 47 Weight: [0.87987821 0.50889916] Bias: -0.3102890084936186\n",
            "Round: 48 Weight: [0.89092212 0.51477973] Bias: -0.3164629712460084\n",
            "Round: 49 Weight: [0.90180738 0.52056546] Bias: -0.32253405241732996\n",
            "Round: 50 Weight: [0.91253831 0.52625919] Bias: -0.3285049606794923\n",
            "Round: 51 Weight: [0.92311904 0.53186366] Bias: -0.33437830804729934\n",
            "Round: 52 Weight: [0.93355355 0.53738149] Bias: -0.3401566141116394\n",
            "Round: 53 Weight: [0.94384565 0.54281519] Bias: -0.34584231006542054\n",
            "Round: 54 Weight: [0.95399902 0.54816716] Bias: -0.3514377425321588\n",
            "Round: 55 Weight: [0.96401721 0.55343972] Bias: -0.3569451772068393\n",
            "Round: 56 Weight: [0.9739036  0.55863508] Bias: -0.36236680231835267\n",
            "Round: 57 Weight: [0.9836615  0.56375537] Bias: -0.36770473192246406\n",
            "Round: 58 Weight: [0.99329404 0.56880264] Bias: -0.3729610090339173\n",
            "Round: 59 Weight: [1.00280428 0.57377885] Bias: -0.378137608605911\n",
            "Round: 60 Weight: [1.01219516 0.57868589] Bias: -0.38323644036481647\n",
            "Round: 61 Weight: [1.0214695  0.58352558] Bias: -0.38825935150764423\n",
            "Round: 62 Weight: [1.03063004 0.58829967] Bias: -0.39320812926940396\n",
            "Round: 63 Weight: [1.03967942 0.59300984] Bias: -0.39808450336715306\n",
            "Round: 64 Weight: [1.04862018 0.59765771] Bias: -0.40289014832718545\n",
            "Round: 65 Weight: [1.05745478 0.60224485] Bias: -0.40762668570148236\n",
            "Round: 66 Weight: [1.0661856  0.60677276] Bias: -0.41229568617922735\n",
            "Round: 67 Weight: [1.07481494 0.61124289] Bias: -0.4168986715988816\n",
            "Round: 68 Weight: [1.08334503 0.61565664] Bias: -0.4214371168660223\n",
            "Round: 69 Weight: [1.091778   0.62001536] Bias: -0.42591245178186743\n",
            "Round: 70 Weight: [1.10011594 0.62432036] Bias: -0.4303260627871425\n",
            "Round: 71 Weight: [1.10836086 0.62857289] Bias: -0.4346792946256919\n",
            "Round: 72 Weight: [1.11651472 0.63277417] Bias: -0.43897345193199566\n",
            "Round: 73 Weight: [1.1245794  0.63692536] Bias: -0.4432098007465248\n",
            "Round: 74 Weight: [1.13255673 0.64102762] Bias: -0.44738956996265017\n",
            "Round: 75 Weight: [1.14044848 0.64508203] Bias: -0.45151395270861705\n",
            "Round: 76 Weight: [1.14825638 0.64908965] Bias: -0.4555841076679013\n",
            "Round: 77 Weight: [1.15598209 0.65305151] Bias: -0.4596011603410819\n",
            "Round: 78 Weight: [1.16362722 0.65696861] Bias: -0.4635662042521905\n",
            "Round: 79 Weight: [1.17119335 0.6608419 ] Bias: -0.46748030210233527\n",
            "Round: 80 Weight: [1.178682   0.66467232] Bias: -0.4713444868732435\n",
            "Round: 81 Weight: [1.18609465 0.66846076] Bias: -0.47515976288321987\n",
            "Round: 82 Weight: [1.19343274 0.67220811] Bias: -0.4789271067978829\n",
            "Round: 83 Weight: [1.20069766 0.6759152 ] Bias: -0.4826474685979108\n",
            "Round: 84 Weight: [1.20789077 0.67958286] Bias: -0.48632177250590714\n",
            "Round: 85 Weight: [1.21501338 0.68321189] Bias: -0.4899509178743823\n",
            "Round: 86 Weight: [1.22206678 0.68680305] Bias: -0.49353578003673715\n",
            "Round: 87 Weight: [1.22905221 0.6903571 ] Bias: -0.49707721112303577\n",
            "Round: 88 Weight: [1.23597088 0.69387477] Bias: -0.5005760408422549\n",
            "Round: 89 Weight: [1.24282398 0.69735675] Bias: -0.5040330772326109\n",
            "Round: 90 Weight: [1.24961266 0.70080373] Bias: -0.5074491073814762\n",
            "Round: 91 Weight: [1.25633802 0.70421639] Bias: -0.5108248981163197\n",
            "Round: 92 Weight: [1.26300116 0.70759536] Bias: -0.5141611966680277\n",
            "Round: 93 Weight: [1.26960314 0.71094128] Bias: -0.5174587313078922\n",
            "Round: 94 Weight: [1.27614499 0.71425476] Bias: -0.5207182119594842\n",
            "Round: 95 Weight: [1.28262771 0.71753638] Bias: -0.5239403307865679\n",
            "Round: 96 Weight: [1.28905229 0.72078674] Bias: -0.5271257627581512\n",
            "Round: 97 Weight: [1.29541968 0.72400638] Bias: -0.53027516619171\n",
            "Round: 98 Weight: [1.30173082 0.72719587] Bias: -0.5333891832755735\n",
            "Round: 99 Weight: [1.3079866  0.73035572] Bias: -0.5364684405714036\n",
            "Round: 100 Weight: [1.31418793 0.73348646] Bias: -0.5395135494976575\n",
            "Round: 101 Weight: [1.32033566 0.7365886 ] Bias: -0.5425251067948753\n",
            "Round: 102 Weight: [1.32643063 0.73966261] Bias: -0.5455036949735934\n",
            "Round: 103 Weight: [1.33247368 0.74270899] Bias: -0.5484498827456438\n",
            "Round: 104 Weight: [1.33846559 0.7457282 ] Bias: -0.5513642254395619\n",
            "Round: 105 Weight: [1.34440717 0.7487207 ] Bias: -0.5542472654007895\n",
            "Round: 106 Weight: [1.35029918 0.75168691] Bias: -0.5570995323773266\n",
            "Round: 107 Weight: [1.35614236 0.75462729] Bias: -0.5599215438914519\n",
            "Round: 108 Weight: [1.36193745 0.75754224] Bias: -0.5627138055981045\n",
            "Round: 109 Weight: [1.36768516 0.76043219] Bias: -0.5654768116304876\n",
            "Round: 110 Weight: [1.37338619 0.76329752] Bias: -0.5682110449334306\n",
            "Round: 111 Weight: [1.37904122 0.76613863] Bias: -0.5709169775850187\n",
            "Round: 112 Weight: [1.38465093 0.76895591] Bias: -0.5735950711069759\n",
            "Round: 113 Weight: [1.39021596 0.77174972] Bias: -0.5762457767642637\n",
            "Round: 114 Weight: [1.39573696 0.77452042] Bias: -0.5788695358543354\n",
            "Round: 115 Weight: [1.40121454 0.77726838] Bias: -0.5814667799864687\n",
            "Round: 116 Weight: [1.40664933 0.77999394] Bias: -0.5840379313515742\n",
            "Round: 117 Weight: [1.41204191 0.78269744] Bias: -0.5865834029828642\n",
            "Round: 118 Weight: [1.41739288 0.7853792 ] Bias: -0.5891035990077449\n",
            "Round: 119 Weight: [1.4227028  0.78803956] Bias: -0.5915989148912816\n",
            "Round: 120 Weight: [1.42797225 0.79067882] Bias: -0.5940697376715667\n",
            "Round: 121 Weight: [1.43320176 0.7932973 ] Bias: -0.5965164461873099\n",
            "Round: 122 Weight: [1.43839187 0.7958953 ] Bias: -0.5989394112979527\n",
            "Round: 123 Weight: [1.44354312 0.79847311] Bias: -0.6013389960965954\n",
            "Round: 124 Weight: [1.44865602 0.80103102] Bias: -0.6037155561160149\n",
            "Round: 125 Weight: [1.45373107 0.80356931] Bias: -0.6060694395280368\n",
            "Round: 126 Weight: [1.45876877 0.80608827] Bias: -0.608400987336514\n",
            "Round: 127 Weight: [1.46376961 0.80858815] Bias: -0.6107105335641545\n",
            "Round: 128 Weight: [1.46873406 0.81106923] Bias: -0.6129984054334287\n",
            "Round: 129 Weight: [1.47366259 0.81353177] Bias: -0.6152649235417778\n",
            "Round: 130 Weight: [1.47855565 0.81597601] Bias: -0.617510402031335\n",
            "Round: 131 Weight: [1.4834137 0.8184022] Bias: -0.6197351487533623\n",
            "Round: 132 Weight: [1.48823718 0.82081059] Bias: -0.6219394654275963\n",
            "Round: 133 Weight: [1.49302651 0.82320142] Bias: -0.6241236477966892\n",
            "Round: 134 Weight: [1.49778213 0.82557492] Bias: -0.6262879857759239\n",
            "Round: 135 Weight: [1.50250444 0.82793131] Bias: -0.6284327635983727\n",
            "Round: 136 Weight: [1.50719385 0.83027082] Bias: -0.6305582599556631\n",
            "Round: 137 Weight: [1.51185077 0.83259368] Bias: -0.6326647481345081\n",
            "Round: 138 Weight: [1.51647558 0.83490008] Bias: -0.6347524961491521\n",
            "Round: 139 Weight: [1.52106867 0.83719026] Bias: -0.6368217668698739\n",
            "Round: 140 Weight: [1.52563043 0.8394644 ] Bias: -0.6388728181476887\n",
            "Round: 141 Weight: [1.53016122 0.84172271] Bias: -0.6409059029353784\n",
            "Round: 142 Weight: [1.5346614 0.8439654] Bias: -0.6429212694049806\n",
            "Round: 143 Weight: [1.53913134 0.84619265] Bias: -0.6449191610618563\n",
            "Round: 144 Weight: [1.54357138 0.84840466] Bias: -0.6468998168554554\n",
            "Round: 145 Weight: [1.54798188 0.85060162] Bias: -0.6488634712868917\n",
            "Round: 146 Weight: [1.55236317 0.8527837 ] Bias: -0.6508103545134365\n",
            "Round: 147 Weight: [1.55671558 0.85495109] Bias: -0.6527406924500344\n",
            "Round: 148 Weight: [1.56103945 0.85710396] Bias: -0.6546547068679419\n",
            "Round: 149 Weight: [1.56533509 0.8592425 ] Bias: -0.6565526154905846\n",
            "Round: 150 Weight: [1.56960283 0.86136686] Bias: -0.6584346320867255\n",
            "Round: 151 Weight: [1.57384297 0.86347721] Bias: -0.6603009665610334\n",
            "Round: 152 Weight: [1.57805582 0.86557373] Bias: -0.6621518250421379\n",
            "Round: 153 Weight: [1.58224168 0.86765657] Bias: -0.6639874099682507\n",
            "Round: 154 Weight: [1.58640084 0.86972588] Bias: -0.6658079201704361\n",
            "Round: 155 Weight: [1.59053361 0.87178184] Bias: -0.6676135509536038\n",
            "Round: 156 Weight: [1.59464025 0.87382457] Bias: -0.6694044941752986\n",
            "Round: 157 Weight: [1.59872106 0.87585425] Bias: -0.6711809383223583\n",
            "Round: 158 Weight: [1.60277632 0.87787101] Bias: -0.6729430685855065\n",
            "Round: 159 Weight: [1.60680628 0.879875  ] Bias: -0.6746910669319469\n",
            "Round: 160 Weight: [1.61081123 0.88186636] Bias: -0.6764251121760217\n",
            "Round: 161 Weight: [1.61479143 0.88384524] Bias: -0.6781453800479952\n",
            "Round: 162 Weight: [1.61874712 0.88581176] Bias: -0.6798520432610209\n",
            "Round: 163 Weight: [1.62267858 0.88776607] Bias: -0.6815452715763495\n",
            "Round: 164 Weight: [1.62658605 0.88970829] Bias: -0.6832252318668306\n",
            "Round: 165 Weight: [1.63046978 0.89163856] Bias: -0.6848920881787631\n",
            "Round: 166 Weight: [1.63433001 0.89355701] Bias: -0.6865460017921423\n",
            "Round: 167 Weight: [1.63816698 0.89546376] Bias: -0.6881871312793545\n",
            "Round: 168 Weight: [1.64198093 0.89735894] Bias: -0.6898156325623657\n",
            "Round: 169 Weight: [1.64577209 0.89924266] Bias: -0.6914316589684497\n",
            "Round: 170 Weight: [1.64954068 0.90111506] Bias: -0.6930353612844999\n",
            "Round: 171 Weight: [1.65328694 0.90297624] Bias: -0.6946268878099668\n",
            "Round: 172 Weight: [1.65701109 0.90482632] Bias: -0.6962063844084623\n",
            "Round: 173 Weight: [1.66071334 0.90666542] Bias: -0.6977739945580705\n",
            "Round: 174 Weight: [1.66439391 0.90849366] Bias: -0.6993298594004035\n",
            "Round: 175 Weight: [1.66805301 0.91031113] Bias: -0.7008741177884376\n",
            "Round: 176 Weight: [1.67169085 0.91211795] Bias: -0.7024069063331678\n",
            "Round: 177 Weight: [1.67530764 0.91391423] Bias: -0.7039283594491133\n",
            "Round: 178 Weight: [1.67890358 0.91570008] Bias: -0.7054386093987071\n",
            "Round: 179 Weight: [1.68247886 0.91747559] Bias: -0.706937786335604\n",
            "Round: 180 Weight: [1.68603369 0.91924087] Bias: -0.7084260183469356\n",
            "Round: 181 Weight: [1.68956827 0.92099602] Bias: -0.7099034314945438\n",
            "Round: 182 Weight: [1.69308277 0.92274114] Bias: -0.7113701498552213\n",
            "Round: 183 Weight: [1.6965774  0.92447633] Bias: -0.7128262955599878\n",
            "Round: 184 Weight: [1.70005233 0.92620168] Bias: -0.7142719888324285\n",
            "Round: 185 Weight: [1.70350775 0.92791729] Bias: -0.7157073480261213\n",
            "Round: 186 Weight: [1.70694384 0.92962326] Bias: -0.7171324896611789\n",
            "Round: 187 Weight: [1.71036079 0.93131967] Bias: -0.7185475284599289\n",
            "Round: 188 Weight: [1.71375875 0.93300661] Bias: -0.7199525773817576\n",
            "Round: 189 Weight: [1.71713792 0.93468418] Bias: -0.7213477476571395\n",
            "Round: 190 Weight: [1.72049845 0.93635246] Bias: -0.7227331488208749\n",
            "Round: 191 Weight: [1.72384052 0.93801155] Bias: -0.7241088887445575\n",
            "Round: 192 Weight: [1.7271643  0.93966151] Bias: -0.7254750736682928\n",
            "Round: 193 Weight: [1.73046994 0.94130245] Bias: -0.726831808231688\n",
            "Round: 194 Weight: [1.73375762 0.94293444] Bias: -0.7281791955041327\n",
            "Round: 195 Weight: [1.73702748 0.94455756] Bias: -0.729517337014389\n",
            "Round: 196 Weight: [1.74027969 0.9461719 ] Bias: -0.7308463327795113\n",
            "Round: 197 Weight: [1.74351441 0.94777754] Bias: -0.7321662813331106\n",
            "Round: 198 Weight: [1.74673178 0.94937454] Bias: -0.7334772797529844\n",
            "Round: 199 Weight: [1.74993195 0.950963  ] Bias: -0.7347794236881259\n",
            "Round: 200 Weight: [1.75311509 0.95254299] Bias: -0.73607280738513\n",
            "Round: 201 Weight: [1.75628133 0.95411458] Bias: -0.7373575237140131\n",
            "Round: 202 Weight: [1.75943082 0.95567785] Bias: -0.738633664193459\n",
            "Round: 203 Weight: [1.7625637  0.95723286] Bias: -0.7399013190155096\n",
            "Round: 204 Weight: [1.76568012 0.95877971] Bias: -0.7411605770697114\n",
            "Round: 205 Weight: [1.76878022 0.96031844] Bias: -0.7424115259667342\n",
            "Round: 206 Weight: [1.77186413 0.96184914] Bias: -0.7436542520614742\n",
            "Round: 207 Weight: [1.77493199 0.96337187] Bias: -0.7448888404756552\n",
            "Round: 208 Weight: [1.77798393 0.96488671] Bias: -0.7461153751199409\n",
            "Round: 209 Weight: [1.7810201  0.96639371] Bias: -0.7473339387155701\n",
            "Round: 210 Weight: [1.78404061 0.96789295] Bias: -0.7485446128155275\n",
            "Round: 211 Weight: [1.7870456 0.9693845] Bias: -0.7497474778252617\n",
            "Round: 212 Weight: [1.7900352  0.97086841] Bias: -0.7509426130229608\n",
            "Round: 213 Weight: [1.79300953 0.97234475] Bias: -0.7521300965793984\n",
            "Round: 214 Weight: [1.79596873 0.97381359] Bias: -0.7533100055773589\n",
            "Round: 215 Weight: [1.7989129  0.97527499] Bias: -0.7544824160306542\n",
            "Round: 216 Weight: [1.80184218 0.976729  ] Bias: -0.7556474029027402\n",
            "Round: 217 Weight: [1.80475668 0.97817569] Bias: -0.7568050401249447\n",
            "Round: 218 Weight: [1.80765653 0.97961513] Bias: -0.7579554006143143\n",
            "Round: 219 Weight: [1.81054184 0.98104736] Bias: -0.759098556291092\n",
            "Round: 220 Weight: [1.81341273 0.98247245] Bias: -0.7602345780958316\n",
            "Round: 221 Weight: [1.81626931 0.98389046] Bias: -0.7613635360061604\n",
            "Round: 222 Weight: [1.8191117  0.98530144] Bias: -0.762485499053197\n",
            "Round: 223 Weight: [1.82194    0.98670545] Bias: -0.7636005353376333\n",
            "Round: 224 Weight: [1.82475433 0.98810255] Bias: -0.7647087120454883\n",
            "Round: 225 Weight: [1.8275548  0.98949278] Bias: -0.7658100954635415\n",
            "Round: 226 Weight: [1.83034152 0.99087621] Bias: -0.7669047509944544\n",
            "Round: 227 Weight: [1.83311459 0.99225289] Bias: -0.7679927431715861\n",
            "Round: 228 Weight: [1.83587412 0.99362287] Bias: -0.7690741356735113\n",
            "Round: 229 Weight: [1.83862022 0.99498621] Bias: -0.7701489913382478\n",
            "Round: 230 Weight: [1.84135298 0.99634295] Bias: -0.7712173721771987\n",
            "Round: 231 Weight: [1.84407251 0.99769315] Bias: -0.7722793393888184\n",
            "Round: 232 Weight: [1.84677891 0.99903685] Bias: -0.7733349533720072\n",
            "Round: 233 Weight: [1.84947228 1.00037412] Bias: -0.7743842737392405\n",
            "Round: 234 Weight: [1.85215272 1.00170499] Bias: -0.7754273593294402\n",
            "Round: 235 Weight: [1.85482032 1.00302951] Bias: -0.7764642682205932\n",
            "Round: 236 Weight: [1.85747519 1.00434774] Bias: -0.7774950577421234\n",
            "Round: 237 Weight: [1.86011742 1.00565973] Bias: -0.7785197844870212\n",
            "Round: 238 Weight: [1.86274709 1.00696551] Bias: -0.7795385043237393\n",
            "Round: 239 Weight: [1.86536432 1.00826514] Bias: -0.7805512724078563\n",
            "Round: 240 Weight: [1.86796918 1.00955865] Bias: -0.7815581431935164\n",
            "Round: 241 Weight: [1.87056177 1.01084611] Bias: -0.7825591704446484\n",
            "Round: 242 Weight: [1.87314218 1.01212755] Bias: -0.78355440724597\n",
            "Round: 243 Weight: [1.87571049 1.01340302] Bias: -0.7845439060137814\n",
            "Round: 244 Weight: [1.87826681 1.01467255] Bias: -0.7855277185065537\n",
            "Round: 245 Weight: [1.8808112  1.01593621] Bias: -0.7865058958353157\n",
            "Round: 246 Weight: [1.88334377 1.01719402] Bias: -0.7874784884738454\n",
            "Round: 247 Weight: [1.8858646  1.01844602] Bias: -0.7884455462686675\n",
            "Round: 248 Weight: [1.88837376 1.01969228] Bias: -0.7894071184488644\n",
            "Round: 249 Weight: [1.89087135 1.02093281] Bias: -0.7903632536357027\n",
            "Round: 250 Weight: [1.89335745 1.02216767] Bias: -0.79131399985208\n",
            "Round: 251 Weight: [1.89583213 1.0233969 ] Bias: -0.7922594045317956\n",
            "Round: 252 Weight: [1.89829549 1.02462053] Bias: -0.7931995145286496\n",
            "Round: 253 Weight: [1.90074759 1.02583861] Bias: -0.7941343761253732\n",
            "Round: 254 Weight: [1.90318853 1.02705117] Bias: -0.7950640350423943\n",
            "Round: 255 Weight: [1.90561837 1.02825826] Bias: -0.7959885364464423\n",
            "Round: 256 Weight: [1.90803719 1.02945991] Bias: -0.7969079249589947\n",
            "Round: 257 Weight: [1.91044508 1.03065616] Bias: -0.7978222446645697\n",
            "Round: 258 Weight: [1.91284211 1.03184705] Bias: -0.7987315391188681\n",
            "Round: 259 Weight: [1.91522835 1.03303262] Bias: -0.7996358513567661\n",
            "Round: 260 Weight: [1.91760388 1.0342129 ] Bias: -0.8005352239001648\n",
            "Round: 261 Weight: [1.91996878 1.03538793] Bias: -0.8014296987656976\n",
            "Round: 262 Weight: [1.9223231  1.03655774] Bias: -0.8023193174722981\n",
            "Round: 263 Weight: [1.92466694 1.03772238] Bias: -0.8032041210486337\n",
            "Round: 264 Weight: [1.92700035 1.03888187] Bias: -0.8040841500404051\n",
            "Round: 265 Weight: [1.92932342 1.04003626] Bias: -0.8049594445175163\n",
            "Round: 266 Weight: [1.9316362  1.04118557] Bias: -0.8058300440811168\n",
            "Round: 267 Weight: [1.93393877 1.04232984] Bias: -0.806695987870519\n",
            "Round: 268 Weight: [1.9362312  1.04346911] Bias: -0.8075573145699939\n",
            "Round: 269 Weight: [1.93851356 1.04460341] Bias: -0.8084140624154466\n",
            "Round: 270 Weight: [1.94078591 1.04573277] Bias: -0.8092662692009742\n",
            "Round: 271 Weight: [1.94304833 1.04685723] Bias: -0.8101139722853102\n",
            "Round: 272 Weight: [1.94530086 1.04797681] Bias: -0.8109572085981547\n",
            "Round: 273 Weight: [1.94754359 1.04909156] Bias: -0.8117960146463955\n",
            "Round: 274 Weight: [1.94977658 1.05020149] Bias: -0.8126304265202211\n",
            "Round: 275 Weight: [1.95199989 1.05130665] Bias: -0.8134604798991274\n",
            "Round: 276 Weight: [1.95421358 1.05240707] Bias: -0.8142862100578214\n",
            "Round: 277 Weight: [1.95641772 1.05350277] Bias: -0.8151076518720228\n",
            "Round: 278 Weight: [1.95861237 1.05459379] Bias: -0.8159248398241661\n",
            "Round: 279 Weight: [1.96079759 1.05568015] Bias: -0.8167378080090055\n",
            "Round: 280 Weight: [1.96297344 1.0567619 ] Bias: -0.8175465901391242\n",
            "Round: 281 Weight: [1.96513999 1.05783905] Bias: -0.8183512195503493\n",
            "Round: 282 Weight: [1.96729729 1.05891164] Bias: -0.8191517292070754\n",
            "Round: 283 Weight: [1.9694454  1.05997969] Bias: -0.8199481517074982\n",
            "Round: 284 Weight: [1.97158438 1.06104324] Bias: -0.8207405192887601\n",
            "Round: 285 Weight: [1.9737143  1.06210232] Bias: -0.8215288638320082\n",
            "Round: 286 Weight: [1.9758352  1.06315695] Bias: -0.8223132168673684\n",
            "Round: 287 Weight: [1.97794714 1.06420716] Bias: -0.823093609578836\n",
            "Round: 288 Weight: [1.98005019 1.06525297] Bias: -0.8238700728090851\n",
            "Round: 289 Weight: [1.98214439 1.06629443] Bias: -0.8246426370641973\n",
            "Round: 290 Weight: [1.98422981 1.06733155] Bias: -0.8254113325183124\n",
            "Round: 291 Weight: [1.9863065  1.06836435] Bias: -0.8261761890182026\n",
            "Round: 292 Weight: [1.98837451 1.06939288] Bias: -0.8269372360877701\n",
            "Round: 293 Weight: [1.99043389 1.07041715] Bias: -0.8276945029324723\n",
            "Round: 294 Weight: [1.99248471 1.0714372 ] Bias: -0.8284480184436723\n",
            "Round: 295 Weight: [1.99452701 1.07245303] Bias: -0.829197811202921\n",
            "Round: 296 Weight: [1.99656085 1.0734647 ] Bias: -0.8299439094861666\n",
            "Round: 297 Weight: [1.99858627 1.07447221] Bias: -0.8306863412678971\n",
            "Round: 298 Weight: [2.00060334 1.07547559] Bias: -0.8314251342252156\n",
            "Round: 299 Weight: [2.0026121  1.07647488] Bias: -0.8321603157418491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting the label\n",
        "y_pred=predict(X_train)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXGT-pka8KG_",
        "outputId": "9107e865-f1db-4b22-ab53-00f002690294"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 1.\n",
            " 1. 0. 1. 1. 0. 0. 1. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 0. 1. 1.\n",
            " 0. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.\n",
            " 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1.\n",
            " 0. 0. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 0.\n",
            " 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print actual and predicted values in a table\n",
        "print(\"actual Value\\tpredicted values\")\n",
        "for i in range(len(y_test)):\n",
        "    print(y_test[i],\"\\t\\t\",int(y_pred[i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbo3O59j8NgZ",
        "outputId": "6ccdaf0f-fdd5-470b-9305-45a516044f4c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual Value\tpredicted values\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "1 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "1 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n",
            "0 \t\t 1\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "0 \t\t 0\n",
            "1 \t\t 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating accuracy of prediction\n",
        "acc = np.sum([y_test[i] == (y_pred[i]) for i in range(len(y_test))])/len(y_test)\n",
        "print(\"Accuracy:\",acc*100,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1NbLaYi8Q3c",
        "outputId": "feadccc2-1169-4f93-d28d-eec2797ddc6f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 60.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3.1\n",
        "Using sklearn LogisticRegression model"
      ],
      "metadata": {
        "id": "dNlMGOMi8ZBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression()\n",
        "logisticRegr.fit(X_train, y_train)\n",
        "a = logisticRegr.predict(X_test[0].reshape(1,-1))\n",
        "a\n",
        "predictions = logisticRegr.predict(X_test)\n",
        "print(predictions)\n",
        "print(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxipB8pw8lx8",
        "outputId": "5155b9a9-7368-4346-a8b6-8766b5507bd0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0]\n",
            "[0 1 1 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 1 0 1 0 0 1 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0\n",
            " 0 0 0 1 0 0 0 0 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use score method to get accuracy of model\n",
        "score = logisticRegr.score(X_test, y_test)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IR9eHZOj84hu",
        "outputId": "1fd9a297-082e-4397-f3bd-f9ebdbd5c0f6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "class_names = [0,1]\n",
        "fig,ax = plt.subplots()\n",
        "tick_marks = np.arange(len(class_names))\n",
        "plt.xticks(tick_marks,class_names)\n",
        "plt.yticks(tick_marks,class_names)\n",
        "\n",
        "sns.heatmap(pd.DataFrame(cnf_matrix),annot=True, cmap=\"YlGnBu\",fmt='g')\n",
        "ax.xaxis.set_label_position(\"top\")\n",
        "plt.tight_layout()\n",
        "plt.title('confusion matrix',y=1.1)\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')"
      ],
      "metadata": {
        "id": "UqMqlF3P1TGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "# target_names = ['without diabetes', 'with diabetes']\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "nonrOELx1Zb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_proba = logreg.predict_proba(X_test)[::,1]\n",
        "fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n",
        "auc = metrics.roc_auc_score(y_test, y_pred_proba)\n",
        "plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n",
        "plt.legend(loc=4)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "iDU60VGJ1fb3",
        "outputId": "45a6fa31-bc33-46e4-daa3-500746be3a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset,DataLoader\n",
        "import math\n",
        "# reading the csv file, del 2 columns from the file, checking first few rows of the file\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset = pd.read_csv(\"/content/drive/MyDrive/datasets/BuyComputer.csv\")\n",
        "dataset.drop(columns=['User ID',],axis=1,inplace=True)\n",
        "label = dataset.iloc[:,-1].values\n",
        "X = dataset.drop(\"Purchased\" ,axis= 1)\n",
        "# Splitting data\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,\n",
        "label, test_size = 0.30, random_state = 28)\n",
        "# Sacaling data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_Train = sc.fit_transform(X_Train)\n",
        "X_Test = sc.transform(X_Test)\n",
        "\n",
        "# print(X_Train)\n",
        "X_Train=torch.from_numpy(X_Train)\n",
        "y_Train=torch.from_numpy(y_Train)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logisticRegr = LogisticRegression(random_state = 28)\n",
        "logisticRegr.fit(X_Train, y_Train)\n",
        "predictions = logisticRegr.predict(X_Test)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "id": "nwZSGlMO7Vyr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "366f1a4a-f33b-4474-a55a-8a47840e13df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[0 1 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 1\n",
            " 0 0 1 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 1 1 1 0 0\n",
            " 0 1 0 1 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 0\n",
            " 0 0 1 0 1 0 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression()\n",
        "from sklearn.metrics import  accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "#Fit\n",
        "LR.fit(X_train,y_train)\n",
        "\n",
        "#predicting the test label with LR. Predict always takes X as input\n",
        "y_test_pred = LR.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_test_pred)*100\n",
        "print(\"Accuracy: \", accuracy,\"%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5lnQvnUdJK8",
        "outputId": "9f05e6fe-a969-4845-d2de-96256b7e5108"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  80.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = precision_score(y_test, y_test_pred, average=None)\n",
        "recall = recall_score(y_test, y_test_pred, average=None)\n",
        "print('precision: {}'.format(precision))\n",
        "print('recall: {}'.format(recall))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg6C9o8XdL6t",
        "outputId": "8b809429-ee99-43f4-c775-f73e6a660649"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: [0.83561644 0.7037037 ]\n",
            "recall: [0.88405797 0.61290323]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "l1=np.array(l1)"
      ],
      "metadata": {
        "id": "BJeoX_DndMlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(l1)\n",
        "     "
      ],
      "metadata": {
        "id": "soOnJ55FdQXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l1"
      ],
      "metadata": {
        "id": "bLBk4zl_dSwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=np.array(range(0,3000))\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UolVd1yddTSm",
        "outputId": "34da3294-eeeb-489f-ac74-151923ea92a6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([   0,    1,    2, ..., 2997, 2998, 2999])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(x,l1)"
      ],
      "metadata": {
        "id": "KmuOmf8PdWpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hello1=np.array([[28,76000]])\n",
        "d=np.array(hello1)\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFneT04adbPl",
        "outputId": "2653e3d6-5a27-46fb-f07e-65d57a1df4a6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   28, 76000]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_ask=sc.fit(d)\n",
        "x_ask=x_ask.transform(d)\n",
        "x_ask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItZ44rh9dd51",
        "outputId": "29bdf76c-c9a3-4652-a60b-c80777fb4291"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LR.predict(x_ask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XXKxo6Xdir7",
        "outputId": "2c4d0252-432d-4466-9049-e88535e908ef"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}